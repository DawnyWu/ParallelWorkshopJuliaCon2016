{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI and Elemental in Julia\n",
    "## Parallel Workshop JuliaCon 2016\n",
    "### `MPI.jl`\n",
    "- MPI.jl provides\n",
    " - Julia wrappers for many MPI function but not yet all the newer ones. Normal script execution, e.g.\n",
    " ```\n",
    " mpirun -np 100000 julia mpiprogram.jl\n",
    " ```\n",
    " - An MPI Cluster manager for interactive execution of MPI jobs. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Recompiling stale cache file /home/juser/.julia/lib/v0.4/MPI.ji for module MPI.\n"
     ]
    }
   ],
   "source": [
    "using MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `MPIManager` and use `addprocs` to launch the workers. This will automatically initialize MPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man = MPIManager(np = 8)\n",
    "addprocs(man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a command on the MPI workers, use the `@mpi_do` macro. Here, store the MPI rank in a variable and `@show` it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 9:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 7\n",
      "\tFrom worker 8:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 6\n",
      "\tFrom worker 2:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 0\n",
      "\tFrom worker 3:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 1\n",
      "\tFrom worker 7:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 5\n",
      "\tFrom worker 4:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 2\n",
      "\tFrom worker 6:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 4\n",
      "\tFrom worker 5:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 3\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man @show myrank = MPI.Comm_rank(MPI.COMM_WORLD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocate the vector `x` on all workers and show its values. Notice that the RNG is not syncronized accross workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 4:\tx = randn(2) = [-1.6765456730388448,1.2131824359477428]\n",
      "\tFrom worker 5:\tx = randn(2) = [-1.1267065325470114,-0.29528738428502355]\n",
      "\tFrom worker 2:\tx = randn(2) = [-0.19007999177859225,-1.064609349404248]\n",
      "\tFrom worker 8:\tx = randn(2) = [0.9657671133584432,-1.7751556133408686]\n",
      "\tFrom worker 6:\tx = randn(2) = [0.44935705433847023,1.6370970004054133]\n",
      "\tFrom worker 3:\tx = randn(2) = [0.36752292128476544,1.0107201978778455]\n",
      "\tFrom worker 9:\tx = randn(2) = [0.09719767718957066,1.0017534649683444]\n",
      "\tFrom worker 7:\tx = randn(2) = [-1.8769314228769776,-0.6809231908781325]\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man @show x = randn(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show an example of `Bcast!` which is the Julia wrapper function for the collective MPI operation `MPI_Bcast`. The function is overloaded for several input types. The type of the broadcasted buffer is always determined from the Julia type and if the input argument is a Julia vector then the size determined automatically as well. Alternatively if the argument is either a vector or pointer, the length can be specified as an integer argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 7:\tx = [-0.19007999177859225,-1.064609349404248]\n",
      "\tFrom worker 4:\tx = [-0.19007999177859225,-1.064609349404248]\n",
      "\tFrom worker 2:\tx = [-0.19007999177859225,-1.064609349404248]\n",
      "\tFrom worker 9:\tx = [-0.19007999177859225,-1.064609349404248]\n",
      "\tFrom worker 5:\tx = [-0.19007999177859225,-1.064609349404248]\n",
      "\tFrom worker 8:\tx = [-0.19007999177859225,-1.064609349404248]\n",
      "\tFrom worker 6:\tx = [-0.19007999177859225,-1.064609349404248]\n",
      "\tFrom worker 3:\tx = [-0.19007999177859225,-1.064609349404248]\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man MPI.Bcast!(x, 0, MPI.COMM_WORLD)\n",
    "@mpi_do man @show x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 4:\tx = [3.0,-1.0162911416865608]\n",
      "\tFrom worker 6:\tx = [-0.4078192531491544,-1.0162911416865608]\n",
      "\tFrom worker 7:\tx = [-0.4078192531491544,-1.0162911416865608]\n",
      "\tFrom worker 3:\tx = [-0.4078192531491544,-1.0162911416865608]\n",
      "\tFrom worker 5:\tx = [-0.4078192531491544,-1.0162911416865608]\n",
      "\tFrom worker 8:\tx = [-0.4078192531491544,-1.0162911416865608]\n",
      "\tFrom worker 9:\tx = [-0.4078192531491544,-1.0162911416865608]\n",
      "\tFrom worker 2:\tx = [-0.4078192531491544,-1.0162911416865608]\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man begin\n",
    "    if myrank == 3\n",
    "        MPI.Send(3.0, 2, 0, MPI.COMM_WORLD)\n",
    "        elseif myrank == 2\n",
    "        MPI.Recv!(x, 1, 3, 0, MPI.COMM_WORLD)\n",
    "    end\n",
    "end\n",
    "@mpi_do man @show x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Elemental.jl`\n",
    "\n",
    "- [Elemental](http://github.com/Elemental/elemental) is a C++ library for distributed dense linear algebra (lately also some sparse and optimization functions)\n",
    "- `Elemental.jl` provides Julia wrappers for `Elemental`.\n",
    "- Still alpha stage\n",
    "- Two APIs\n",
    " - Thin layer on top of C++ library\n",
    " - Higher level with `DArray` interoperability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Elemental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@mpi_do man n = 2000\n",
    "@mpi_do man A = Elemental.DistMatrix(Float64)\n",
    "@mpi_do man Elemental.gaussian!(A, n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 7:\tA[1,1] = 0.1361529124304835\n",
      "\tFrom worker 4:\tA[1,1] = 0.1361529124304835\n",
      "\tFrom worker 8:\tA[1,1] = 0.1361529124304835\n",
      "\tFrom worker 2:\tA[1,1] = 0.1361529124304835\n",
      "\tFrom worker 5:\tA[1,1] = 0.1361529124304835\n",
      "\tFrom worker 3:\tA[1,1] = 0.1361529124304835\n",
      "\tFrom worker 9:\tA[1,1] = 0.1361529124304835\n",
      "\tFrom worker 6:\tA[1,1] = 0.1361529124304835\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man @show A[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.145919 seconds (5.23 k allocations: 396.407 KB)\n"
     ]
    }
   ],
   "source": [
    "@time @mpi_do man vals = svdvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 2:\tvals[1] = 89.14574736698128\n",
      "\tFrom worker 7:\tvals[1] = 89.14574736698128\n",
      "\tFrom worker 6:\tvals[1] = 89.14574736698128\n",
      "\tFrom worker 5:\tvals[1] = 89.14574736698128\n",
      "\tFrom worker 9:\tvals[1] = 89.14574736698128\n",
      "\tFrom worker 3:\tvals[1] = 89.14574736698128\n",
      "\tFrom worker 4:\tvals[1] = 89.14574736698128\n",
      "\tFrom worker 8:\tvals[1] = 89.14574736698128\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man @show vals[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(svdvals(A))[1] = 89.15005868298738\n",
      "  3.364512 seconds (90.60 k allocations: 35.763 MB, 0.07% gc time)\n"
     ]
    }
   ],
   "source": [
    "n = 2000\n",
    "A = randn(n, n);\n",
    "@time @show svdvals(A)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Checking out TSVD master...\n",
      "INFO: Pulling TSVD latest master...\n",
      "INFO: No packages to install, update or remove\n"
     ]
    }
   ],
   "source": [
    "# Pkg.clone(\"https://github.com/andreasnoack/TSVD.jl\")\n",
    "# Pkg.checkout(\"TSVD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using TSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.552865 seconds (4.93 k allocations: 380.784 KB)\n"
     ]
    }
   ],
   "source": [
    "@time @mpi_do man vals = TSVD.tsvd(A, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.5-pre",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
