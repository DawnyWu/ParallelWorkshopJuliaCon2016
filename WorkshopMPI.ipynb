{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI and Elemental in Julia\n",
    "## Parallel Workshop JuliaCon 2016\n",
    "### `MPI.jl`\n",
    "- MPI.jl provides\n",
    " - Julia wrappers for many MPI function but not yet all the newer ones. Normal script execution, e.g.\n",
    " ```\n",
    " mpirun -np 100000 julia mpiprogram.jl\n",
    " ```\n",
    " - An MPI Cluster manager for interactive execution of MPI jobs. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an `MPIManager` and use `addprocs` to launch the workers. This will automatically initialize MPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "man = MPIManager(np = 8)\n",
    "addprocs(man)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a command on the MPI workers, use the `@mpi_do` macro. Here, store the MPI rank in a variable and `@show` it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 2:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 0\n",
      "\tFrom worker 9:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 7\n",
      "\tFrom worker 3:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 1\n",
      "\tFrom worker 7:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 5\n",
      "\tFrom worker 5:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 3\n",
      "\tFrom worker 8:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 6\n",
      "\tFrom worker 4:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 2\n",
      "\tFrom worker 6:\tmyrank = MPI.Comm_rank(MPI.COMM_WORLD) = 4\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man @show myrank = MPI.Comm_rank(MPI.COMM_WORLD);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allocate the vector `x` on all workers and show its values. Notice that the RNG is not syncronized accross workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 3:\tx = randn(2) = [1.561807228850006,-1.8991612031066882]\n",
      "\tFrom worker 4:\tx = randn(2) = [1.7827074625350516,1.1432842639087033]\n",
      "\tFrom worker 5:\tx = randn(2) = [0.7812067994558566,1.3515494345331944]\n",
      "\tFrom worker 9:\tx = randn(2) = [-1.0100482065707683,0.46991613460582093]\n",
      "\tFrom worker 7:\tx = randn(2) = [-0.4051027120044051,-0.8893470928293499]\n",
      "\tFrom worker 6:\tx = randn(2) = [-0.7893776602217285,-2.3168249675165273]\n",
      "\tFrom worker 2:\tx = randn(2) = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 8:\tx = randn(2) = [0.4709601667537731,-1.1963960727081422]\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man @show x = randn(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send and receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 7:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 4:\tx = [3.0,0.6965055849677959]\n",
      "\tFrom worker 3:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 2:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 5:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 8:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 6:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 9:\tx = [-2.7318205254023775,0.6965055849677959]\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man begin\n",
    "    if myrank == 3\n",
    "        MPI.Send(3.0, 2, 0, MPI.COMM_WORLD)\n",
    "        elseif myrank == 2\n",
    "        MPI.Recv!(x, 1, 3, 0, MPI.COMM_WORLD)\n",
    "    end\n",
    "end\n",
    "@mpi_do man @show x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we show an example of `Bcast!` which is the Julia wrapper function for the collective MPI operation `MPI_Bcast`. The function is overloaded for several input types. The type of the broadcasted buffer is always determined from the Julia type and if the input argument is a Julia vector then the size determined automatically as well. Alternatively if the argument is either a vector or pointer, the length can be specified as an integer argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 7:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 4:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 9:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 2:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 3:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 5:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 8:\tx = [-2.7318205254023775,0.6965055849677959]\n",
      "\tFrom worker 6:\tx = [-2.7318205254023775,0.6965055849677959]\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man MPI.Bcast!(x, 0, MPI.COMM_WORLD)\n",
    "@mpi_do man @show x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Compute $\\pi$ with `MPI_Reduce` (`MPI_Allreduce` available in `MPI.jl` development version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 2:\tpi_final / MPI.Comm_size(MPI.COMM_WORLD) = 3.1414827\n"
     ]
    }
   ],
   "source": [
    "@mpi_do man begin\n",
    "    n = 10^7\n",
    "    \n",
    "    # compute pi locally\n",
    "    pi_local = mapreduce(i -> rand()^2 + rand()^2 < 1, +, 1:n)/n*4\n",
    "    \n",
    "    # combine the results with MPI_Reduce\n",
    "    pi_final = MPI.Reduce(pi_local, MPI.SUM, 0, MPI.COMM_WORLD)\n",
    "\n",
    "    # show the result\n",
    "    if myrank == 0\n",
    "        @show pi_final / MPI.Comm_size(MPI.COMM_WORLD)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Elemental.jl`\n",
    "\n",
    "- [Elemental](http://github.com/Elemental/elemental) is a C++ library for distributed dense linear algebra (lately also some sparse and optimization functions)\n",
    "- `Elemental.jl` provides Julia wrappers for `Elemental`.\n",
    "- Still alpha stage\n",
    "- Two APIs\n",
    " - Thin layer on top of C++ library\n",
    " - Higher level with `DArray` interoperability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Elemental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a 2000x2000 Elemental distributed matrix of Gaussian variates\n",
    "@mpi_do man n = 2000\n",
    "@mpi_do man A = Elemental.DistMatrix(Float64)\n",
    "@mpi_do man Elemental.gaussian!(A, n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 2:\tA[1,1] = -1.0290752082028511\n",
      "\tFrom worker 4:\tA[1,1] = -1.0290752082028511\n",
      "\tFrom worker 3:\tA[1,1] = -1.0290752082028511\n",
      "\tFrom worker 6:\tA[1,1] = -1.0290752082028511\n",
      "\tFrom worker 5:\tA[1,1] = -1.0290752082028511\n",
      "\tFrom worker 9:\tA[1,1] = -1.0290752082028511\n",
      "\tFrom worker 7:\tA[1,1] = -1.0290752082028511\n",
      "\tFrom worker 8:\tA[1,1] = -1.0290752082028511\n"
     ]
    }
   ],
   "source": [
    "# Print the first element\n",
    "@mpi_do man @show A[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  6.251383 seconds (5.46 k allocations: 413.798 KB)\n"
     ]
    }
   ],
   "source": [
    "# Compute the singular values with Elementals singular value solver. Computes all values.\n",
    "@time @mpi_do man vals = svdvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFrom worker 6:\tvals[1] = 89.16743025999959\n",
      "\tFrom worker 4:\tvals[1] = 89.16743025999959\n",
      "\tFrom worker 3:\tvals[1] = 89.16743025999959\n",
      "\tFrom worker 2:\tvals[1] = 89.16743025999959\n",
      "\tFrom worker 7:\tvals[1] = 89.16743025999959\n",
      "\tFrom worker 5:\tvals[1] = 89.16743025999959\n",
      "\tFrom worker 8:\tvals[1] = 89.16743025999959\n",
      "\tFrom worker 9:\tvals[1] = 89.16743025999959\n"
     ]
    }
   ],
   "source": [
    "# Show the largest singular value\n",
    "@mpi_do man @show vals[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(svdvals(A))[1] = 88.76880541870597\n",
      "  5.691730 seconds (90.51 k allocations: 35.761 MB, 0.06% gc time)\n"
     ]
    }
   ],
   "source": [
    "# Now compate to a local compuation with Julia's `svdvals` based on LAPACK\n",
    "n = 2000\n",
    "A = randn(n, n);\n",
    "@time @show svdvals(A)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now try out my TSVD package on an Elemental array\n",
    "\n",
    "# Pkg.clone(\"https://github.com/andreasnoack/TSVD.jl\")\n",
    "using TSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.757477 seconds (5.88 k allocations: 444.171 KB)\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM: The tsvd function uses random seed on all workers which we saw was different for each worker.\n",
    "# SOLUTION: Use Bcast! (or just set the seed on all workers)\n",
    "@time @mpi_do man begin\n",
    "    v0 = randn(n)\n",
    "    MPI.Bcast!(v0, 0, MPI.COMM_WORLD)\n",
    "    \n",
    "    # compute the SVD\n",
    "    vals = TSVD.tsvd(A, 10, initVec = v0)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.5-pre",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
